<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>AI-Driven Robotic Semantic SLAM: Real-Time Depth and Adaptive Mapping for Autonomous Navigation in Complex Environments</title>
  <link rel="icon" type="image/x-icon" href="static/images/78357759.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AI-Driven Robotic Semantic SLAM: Real-Time Depth and Adaptive Mapping for Autonomous Navigation in Complex Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
  <a href="#" target="_blank">Lyes Saad Saoud</a>,</span>
<span class="author-block">
  
<span class="author-block">
  <a href="#" target="_blank">Irfan Hussain</a>
</span>
</div>

<div class="is-size-5 publication-authors">
  <span class="author-block">Khalifa University of Science and Technology, UAE<br>Preprint 2025</span>
  
</div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/LyesSaadSaoud/SAFARI" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Display Image -->
      <div style="position: relative; max-width: 100%; text-align: center;">
        <img 
          src="static/images/main_tro.png" 
          alt="Visual SLAM" 
          style="width: 70%; height: auto; border-radius: 10px;">
      </div>

      <h2 class="subtitle has-text-centered">
        The framework integrates Depth Anything V2 for real-time dense depth estimation, CLIP for zero-shot semantic feature classification, and LLaMA for dynamic mapping adjustments. Depth Anything V2 generates consistent depth maps, while CLIP identifies contextual features (e.g., underwater scenes, clutter, reflective surfaces) to guide optimization. LLaMA refines SLAM parameters based on detected features and scene descriptions, enabling adaptability to dynamic environments such as underwater or cluttered scenarios. The integration of these components within SAFARI ensures robust 3D mapping, improved pose optimization, and enhanced scalability in complex conditions.
      </h2>
    </div>
  </div>
</section>
<!-- End Teaser Image -->


</section>
<!-- End teaser video -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Video 1 -->
      <div class="column is-one-third has-text-centered">
        <video controls width="100%">
          <source src="static/videos/video1.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Video 1: Description of the content.</p>
      </div>

     

      <!-- Video 3 -->
      <div class="column is-one-third has-text-centered">
        <video controls width="100%">
          <source src="static/videos/video3.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <p>Video 3: Description of the content.</p>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
This paper introduces SAFARI (Semantic-Aware Framework for Adaptive and Robust Robotic SLAM Implementation), an AI-driven framework tailored for autonomous navigation in complex and dynamic environments. Traditional SLAM systems often face challenges with dynamic objects, reflective surfaces, and ambiguous structures due to their reliance solely on geometric features.
SAFARI integrates DepthAnythingV2 for real-time depth estimation, CLIP for zero-shot semantic scene understanding, and LLaMA for adaptive optimization. LLaMA enables real-time analysis and dynamic parameter tuning, such as adjusting tracking and mapping iterations based on scene complexity and feature density. A novel semantic-depth fusion module combines semantic insights with geometric data to refine depth maps, optimize keyframe selection, and enhance mapping accuracy.
The framework is evaluated on the Replica and KU Marine Pool (KUMP) datasets, the latter designed to simulate underwater robotic navigation scenarios with real-world challenges such as low visibility, reflective surfaces, and dynamic currents. SAFARI demonstrates superior performance compared to traditional methods, achieving significant improvements in Absolute Trajectory Error (ATE), Relative Pose Error (RPE), and perceptual mapping metrics.
While SAFARI showcases robust performance, challenges persist in handling highly reflective surfaces and achieving precise semantic segmentation. Future work will enhance semantic-depth fusion algorithms, integrate advanced segmentation techniques, and expand the KUMP dataset for broader robotic applications. SAFARI represents a significant advancement in AI-powered SLAM, offering an adaptable and reliable solution for autonomous systems in diverse and challenging environments.        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Results Section -->
<!-- Image Carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Image 3 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig3.png" alt="Visualization of input images, dehazed outputs, and corresponding depth maps" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 3:</strong> Visualization of input images, dehazed outputs, and corresponding depth maps. The first column shows the original input images. The second column presents the dehazed results, and the third column displays the estimated depth maps. These examples highlight improvements in visibility and geometric consistency achieved by the preprocessing and depth estimation steps.
            </h2>
          </div>
        </div>

        <!-- Image 4 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig4.png" alt="Visualization of the estimated trajectory within the pool environment" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 4:</strong> Visualization of the estimated trajectory within the pool environment. The top-left plot shows the top-down view (X-Y plane), illustrating the horizontal movement of the system. The top-right plot displays the side view (X-Z plane), capturing variations in height along the X-axis. The bottom-left plot presents the front view (Y-Z plane), highlighting vertical movements along the Y-axis. The bottom-right 3D plot provides a comprehensive spatial representation of the trajectory. Color variations across all views represent the frame sequence progression, emphasizing transitions and motion patterns throughout the dataset.
            </h2>
          </div>
        </div>

        <!-- Image 5 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig5.png" alt="Results for Room0 of the Replica dataset using segment masks" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 5:</strong> Results for Room0 of the Replica dataset using segment masks. The figure showcases ground truth RGB, ground truth depth, rasterized RGB (PSNR: 34.25), rasterized depth (L1: 0.00), rasterized silhouette, and the rasterized semantic map (IoU: 0.8916).
            </h2>
          </div>
        </div>

        <!-- Image 6 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig6.png" alt="Results for Room0 of the Replica dataset without using segment masks" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 6:</strong> Results for Room0 of the Replica dataset without using segment masks. The figure showcases ground truth RGB, ground truth depth, rasterized RGB (PSNR: 20.18), rasterized depth (L1: 0.23), rasterized silhouette, and the depth difference (L1).
            </h2>
          </div>
        </div>

        <!-- Image 7 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig7.png" alt="Feature confidence trends over time for the underwater dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 7:</strong> Feature confidence trends over time for the underwater dataset. The plot illustrates the confidence variations of key features such as underwater camera, marine plants, and coral reef across time indices.
            </h2>
          </div>
        </div>

        <!-- Image 8 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig8.png" alt="Top 10 features by average confidence for the underwater dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 8:</strong> Top 10 features by average confidence for the underwater dataset. The bar plot highlights the dominance of underwater camera and marine plants as the most confident features.
            </h2>
          </div>
        </div>

        <!-- Image 9 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig9.png" alt="Confidence heatmap showing the relationship between features and recommendations for the underwater dataset" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 9:</strong> Confidence heatmap showing the relationship between features and recommendations for the underwater dataset. The heatmap emphasizes high-confidence features and their corresponding recommendations, providing actionable insights for optimizing underwater mapping performance.
            </h2>
          </div>
        </div>

        <!-- Image 10 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/Fig10.png" alt="Visualization of ground truth and rasterized data at time step 19" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Fig. 10:</strong> Visualization of ground truth and rasterized data at time step 19. The figure includes ground truth RGB, ground truth depth, rasterized RGB (PSNR: 19.53), rasterized depth (L1: 0.09), rasterized silhouette, and depth difference (L1).
            </h2>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Image Carousel -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{HuBotPaper,
  author = {Saad Saoud, Lyes and et al.},
  title = {..},
  year = {2024},
  publisher = {TRO},
  doi = {......},
  url = {https://doi.org/.....}}
        
  @misc{HuBotDataset,
  author = {Saad Saoud, Lyes and et al.},
  title = {....},
  year = {2024},
  publisher = {},
  version = {V1},
  doi = {xxx},
  url = {https://doi.org/....}}
  
        
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Updated Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page provides supplementary materials for the study "<strong>Semantic-Aware SLAM Framework for Dynamic Environments</strong>." 
For additional details, please refer to the <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank">arXiv version of the paper</a> or download the 
<a href="https://www.mendeley.com/" target="_blank">dataset and annotations</a>. 
The code repository for implementation and replication of results can be found on <a href="https://github.com/LyesSaadSaoud/SAFARI" target="_blank">GitHub</a>.

          </p>
          <p>
            For researchers interested in non-invasive wildlife monitoring, HuBot serves as a unique tool that integrates biomimicry with advanced AI algorithms. Please feel free to use and adapt the resources provided here, and kindly cite the paper and dataset in your work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
