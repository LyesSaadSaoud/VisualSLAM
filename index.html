<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study</title>
  <link rel="icon" type="image/x-icon" href="static/images/78357759.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
  <a href="#" target="_blank">Lyes Saad Saoud</a>,</span>
<span class="author-block">
  
<span class="author-block">
  <a href="#" target="_blank">Irfan Hussain</a>
</span>
</div>

<div class="is-size-5 publication-authors">
  <span class="author-block">Khalifa University of Science and Technology, UAE<br>Preprint 2024</span>
  
</div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/LyesSaadSaoud/VSLAM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser Image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Display Image -->
      <div style="position: relative; max-width: 100%; text-align: center;">
        <img 
          src="static/images/main_tro.png" 
          alt="HuBotâ€™s autonomous bird observational system" 
          style="width: 70%; height: auto; border-radius: 10px;">
      </div>

      <h2 class="subtitle has-text-centered">
        Overview of the proposed Semantic-Aware Adaptive 3D Mapping Framework with Vision-Language Reasoning. The
 framework integrates semantic and geometric features for robust 3D mapping and pose optimization. RGB inputs are processed
 with Depth Anything V2 for dense depth estimation and CLIP for zero-shot object classification, enabling semantic context
 analysis. Semantic-Aware Depth Fusion combines geometric consistency with semantic features, improving spatial accuracy
 and adaptability. LLaMa dynamically adjusts mapping parameters, including keyframe selection, pruning, and iterations, based
 on detected features and scene analysis. Gaussian-splatting with multi-channel optimization refines mapping through pose
 adjustments, densification, and redundancy removal. The outputs include 3D reconstructions, pose trajectories, and keyframe
 metrics such as PSNR, RPE, ATE, and SSIM, supporting applications in autonomous navigation, underwater exploration, and
 intelligent robotics.
      </h2>
    </div>
  </div>
</section>
<!-- End Teaser Image -->


</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
This paper introduces an AI-driven semantic-aware SLAM framework that combines Large Language Models (LLMs), CLIP-based zero-shot recognition, real-time dense depth estimation using DepthAnythingV2, and a novel semantic-depth fusion mechanism for enhanced 3D mapping and pose optimization. Unlike traditional SLAM systems that rely exclusively on geometric features, the proposed framework integrates semantic reasoning with geometry-based optimization, enabling robust scene understanding and adaptive mapping in challenging environments such as cluttered indoor/outdoor scenes, dynamic objects, and low-light conditions.
The framework features a semantic-depth fusion module that refines depth maps by dynamically adjusting confidence values based on the semantic classification of corresponding image features. Real-time dense depth estimation is achieved using DepthAnythingV2, eliminating the need for offline preprocessing and enabling adaptability to dynamic scenes. Context-aware feature classification via CLIP and semantic reasoning through LLMs improve keyframe selection and enhance mapping reliability, particularly in the presence of occlusions, reflective surfaces, and structural ambiguities.
Experiments on the Replica dataset using 200-frame sequences demonstrate promising performance in absolute trajectory error (ATE), relative pose error (RPE), and perceptual metrics such as PSNR and IoU. Notably, the integration of semantic-depth fusion achieves higher depth consistency and improved spatial accuracy. However, results reveal limitations in handling reflective surfaces and achieving high segmentation accuracy (IoU remains low without segment masks). Ablation studies further validate the contributions of semantic-depth fusion and AI-guided perception, emphasizing scalability and adaptability to diverse scenarios.
While the presented results provide a strong proof of concept, limitations in dataset size and experimental scope are acknowledged. Future work will focus on expanding experimental validation, addressing reflective surface challenges, and enhancing semantic segmentation capabilities. This work advances semantic SLAM by bridging AI-driven perception with real-time 3D mapping, offering a foundation for significant improvements in autonomous navigation, robotics, and augmented reality applications.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Results Section -->
<!-- Image Carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

        <!-- Image 3 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image71.png" alt="Qualitative comparison of segmentation results" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 1:</strong> Qualitative comparison of segmentation results. This figure compares the segmentation performance of various models on sample images, highlighting differences in segmentation quality and accuracy. (a) Input image, (b) ground truth, (c) DeepLabV3, (d) LRASPP, (e) FCN, (f) Mask R-CNN, (g) YOLOv5-Seg, (h) YOLOv8-Seg, (i) Detectron2, (j) MobileSAM Baseline, and (k) YOLOv9+MobileSAM.
            </h2>
          </div>
        </div>

        <!-- Image 4 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image61.png" alt="Comparative visual illustration of state-of-the-art depth estimation methods" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 2:</strong> A comparative visual illustration of state-of-the-art depth estimation methods applied to sample images from the depth dataset. This comparison includes the local-depth estimation approach implemented in the Hubot alongside other baseline methods. Sample images span a range of lighting conditions for assessing their impact on depth-estimation performance.
            </h2>
          </div>
        </div>

        <!-- Image 5 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image51.png" alt="Processed images of Houbara bustards demonstrating detection and positioning" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 3:</strong> Processed images of Houbara bustards demonstrating detection and relative positioning within the field of view. The center of gravity of the image is marked with a blue cross, whereas the detected Houbara is highlighted with a red mask.
            </h2>
          </div>
        </div>

        <!-- Image 6 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image31.png" alt="Examples of HuBotâ€™s movements across various environments" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 4:</strong> Examples of HuBotâ€™s movements across various environments: Top row: Feeding motion sequence demonstrating biomimetic movements during feeding behaviors. Second row: Head left and right motion showcasing head-orientation adjustments, mimicking natural head movements. Third row: Movement on a flat surface illustrating stability and navigation on even terrain. Fourth row: Movement on a grass surface highlighting adaptability to grassy terrains. Bottom row: Movement on a granular surface depicting maneuverability on sandy and granular surfaces. These images demonstrate HuBotâ€™s operational capabilities under different conditions, enhancing understanding of its observations with varied environments.
            </h2>
          </div>
        </div>

        <!-- Image 7 -->
        <div class="item">
          <div style="display: flex; justify-content: center; align-items: center; flex-direction: column; text-align: center;">
            <img src="static/images/image41.png" alt="HuBotâ€™s preliminary trials with Houbara bustards in captivity" style="max-width: 80%; height: auto; border-radius: 10px;">
            <h2 class="subtitle has-text-centered">
              <strong>Figure 5:</strong> HuBotâ€™s preliminary trials with Houbara bustards in captivity. Top row: interaction with Female Houbara bustard, reacting with avoidance and cautious behavior. Second row: Male performing sexual courtship behavior directed at the HuBot. Third row: Male exhibiting a cautious stance, closely observing HuBotâ€™s movements without closely approaching and interacting. Bottom Row: Male performing behaviors similar to the first male, including courtship display behavior. These examples illustrate varied behavioral responses of captive Houbara bustards toward the HuBot, linked to sex, individual personality, and, for males, individual differences in sexual motivation.
            </h2>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End Image Carousel -->




<!-- First YouTube Video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Embed YouTube video -->
      <div style="max-width: 800px; margin: auto; position: relative; width: 100%; height: 0; padding-bottom: 56.25%; overflow: hidden;">
        <iframe 
          src="https://www.youtube.com1/embed/kjZdn26Ckic" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
        </iframe>
      </div>
      <h2 class="subtitle has-text-centered">
        Can't see the video? <a href="https://www.youtube.com1/watch?v=kjZdn26Ckic" target="_blank">Watch it on YouTube</a>.
      </h2>
    </div>
  </div>
</section>

<!-- Second YouTube Video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Embed YouTube video -->
      <div style="max-width: 800px; margin: auto; position: relative; width: 100%; height: 0; padding-bottom: 56.25%; overflow: hidden;">
        <iframe 
          src="https://www.youtube.com1/embed/HaQJ1Bb-T6g" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen 
          style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
        </iframe>
      </div>
      <h2 class="subtitle has-text-centered">
        Watch HuBot in Action. If you can't see the video, <a href="https://www.youtube.com1/watch?v=HaQJ1Bb-T6g" target="_blank">click here to watch it on YouTube</a>.
      </h2>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
  @article{HuBotPaper,
  author = {Saad Saoud, Lyes and et al.},
  title = {..},
  year = {2024},
  publisher = {Ecological Informatics},
  doi = {......},
  url = {https://doi.org/.....}}
        
  @misc{HuBotDataset,
  author = {Saad Saoud, Lyes and et al.},
  title = {....},
  year = {2024},
  publisher = {},
  version = {V1},
  doi = {10.17632/tx3vrvsrgv.1},
  url = {https://doi.org/....}}
  
        
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Updated Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page provides supplementary materials for the study "<strong>HuBot: A Biomimicking Mobile Robot for Non-Disruptive Bird Behavior Study</strong>." 
            For additional details, please refer to the <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank">arXiv version of the paper</a> or download the 
            <a href="https://www.mendeley.com/datasets/tx3vrvsrgv/1" target="_blank">dataset and annotations</a>. 
            The code repository for implementation and replication of results can be found on <a href="https://github.com/LyesSaadSaoud/HuBotDetSeg" target="_blank">GitHub</a>.
          </p>
          <p>
            For researchers interested in non-invasive wildlife monitoring, HuBot serves as a unique tool that integrates biomimicry with advanced AI algorithms. Please feel free to use and adapt the resources provided here, and kindly cite the paper and dataset in your work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
